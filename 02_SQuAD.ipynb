{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraniayudyaa/finnalterm-dl/blob/main/02_SQuAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE-TUNING HUGGINGFACE MODELS (SQuAD)**"
      ],
      "metadata": {
        "id": "oNDJaOBzzuxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Setup dan Instalasi**"
      ],
      "metadata": {
        "id": "53m2R6z8z_I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup\n",
        "!pip install transformers datasets torch evaluate nltk rouge-score\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "Ru2xyaNz0Bdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load Dataset**"
      ],
      "metadata": {
        "id": "mNoAJ5MB0CPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Dataset - SQuAD v2.0\n",
        "print(\"Loading SQuAD dataset...\")\n",
        "dataset = load_dataset(\"rajpurkar/squad\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)\n",
        "print(f\"Train samples: {len(dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
        "\n",
        "# 3. Examine data structure\n",
        "print(\"\\nSample from training set:\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Context: {sample['context'][:200]}...\")\n",
        "print(f\"Question: {sample['question']}\")\n",
        "print(f\"Answer: {sample['answers']['text'][0]}\")\n",
        "print(f\"Answer start: {sample['answers']['answer_start'][0]}\")\n",
        "\n",
        "print(\"\\nSample from validation set:\")\n",
        "sample_val = dataset['validation'][0]\n",
        "print(f\"Context: {sample_val['context'][:200]}...\")\n",
        "print(f\"Question: {sample_val['question']}\")\n",
        "print(f\"Answer: {sample_val['answers']['text'][0]}\")"
      ],
      "metadata": {
        "id": "J7MbUmjb0GcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Preprocessing Data**"
      ],
      "metadata": {
        "id": "Vemz5yNL0HD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Preprocess Function for T5\n",
        "MODEL_NAME = \"t5-base\"  # Using t5-base as specified in the task\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def preprocess_squad_for_t5(examples):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for context, question, answers in zip(examples['context'], examples['question'], examples['answers']):\n",
        "        # Format: \"question: {question} context: {context}\"\n",
        "        input_text = f\"question: {question} context: {context}\"\n",
        "        inputs.append(input_text)\n",
        "\n",
        "        # Get answer text (take first answer if multiple)\n",
        "        if len(answers['text']) > 0:\n",
        "            target_text = answers['text'][0]\n",
        "        else:\n",
        "            target_text = \"\"  # For unanswerable questions\n",
        "        targets.append(target_text)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=384,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Tokenize targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# 5. Process Dataset\n",
        "print(\"\\nPreprocessing dataset...\")\n",
        "# Take subset for faster training (remove [:] for full dataset)\n",
        "train_dataset = dataset['train'].select(range(5000))  # Reduced for demo\n",
        "val_dataset = dataset['validation'].select(range(1000))\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_squad_for_t5, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val = val_dataset.map(preprocess_squad_for_t5, batched=True, remove_columns=val_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "9Ixhn7-d0JRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Load Model dan Training**"
      ],
      "metadata": {
        "id": "YW-G9S-q0Jy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Load Model\n",
        "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 7. Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# 8. Load Evaluation Metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decode predictions\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "\n",
        "    # BLEU score\n",
        "    bleu_result = bleu.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=[[ref] for ref in decoded_labels]\n",
        "    )\n",
        "\n",
        "    # Exact Match\n",
        "    exact_matches = sum([1 for p, l in zip(decoded_preds, decoded_labels) if p.strip() == l.strip()])\n",
        "    exact_match = exact_matches / len(decoded_preds)\n",
        "\n",
        "    # F1 Score (approximate)\n",
        "    def compute_f1(pred, gold):\n",
        "        pred_tokens = pred.lower().split()\n",
        "        gold_tokens = gold.lower().split()\n",
        "\n",
        "        common = set(pred_tokens) & set(gold_tokens)\n",
        "\n",
        "        if len(common) == 0:\n",
        "            return 0\n",
        "\n",
        "        precision = len(common) / len(pred_tokens)\n",
        "        recall = len(common) / len(gold_tokens)\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "        return f1\n",
        "\n",
        "    f1_scores = [compute_f1(p, l) for p, l in zip(decoded_preds, decoded_labels)]\n",
        "    avg_f1 = np.mean(f1_scores) if f1_scores else 0\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rouge2\": rouge_result[\"rouge2\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"exact_match\": exact_match,\n",
        "        \"f1\": avg_f1\n",
        "    }\n",
        "\n",
        "# 9. Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results_squad\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    learning_rate=3e-4,\n",
        "    per_device_train_batch_size=4,  # Reduced for memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    logging_steps=100,\n",
        "    gradient_accumulation_steps=2  # For effective batch size of 8\n",
        ")\n",
        "\n",
        "# 10. Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 11. Train Model\n",
        "print(\"\\nTraining model...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Fc20wIsZ0NBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Evaluasi**"
      ],
      "metadata": {
        "id": "QarDdLIf0NjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJspTtJUyK5p"
      },
      "outputs": [],
      "source": [
        "# 12. Evaluate\n",
        "print(\"\\nEvaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"\\nEvaluation results:\")\n",
        "for key, value in eval_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# 13. Save Model\n",
        "print(\"\\nSaving model...\")\n",
        "trainer.save_model(\"./saved_model_t5_squad\")\n",
        "tokenizer.save_pretrained(\"./saved_model_t5_squad\")\n",
        "\n",
        "# 14. Inference Function\n",
        "def answer_question(context, question, model, tokenizer, max_answer_length=50):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=384\n",
        "    )\n",
        "\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=max_answer_length,\n",
        "            min_length=1,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=2,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# 15. Test Examples\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Question Answering Examples\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_examples = [\n",
        "    {\n",
        "        \"context\": \"\"\"\n",
        "        The University of Cambridge is a public collegiate research university in Cambridge, England.\n",
        "        Founded in 1209, the University of Cambridge is the world's third-oldest university in continuous operation.\n",
        "        The university's founding followed the arrival of scholars who left the University of Oxford for Cambridge after a dispute with local townspeople.\n",
        "        \"\"\",\n",
        "        \"question\": \"When was the University of Cambridge founded?\",\n",
        "        \"expected_answer\": \"1209\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\"\"\n",
        "        Machine learning is a field of artificial intelligence that uses statistical techniques to give\n",
        "        computer systems the ability to learn from data, without being explicitly programmed.\n",
        "        The name machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer\n",
        "        in the field of computer gaming and artificial intelligence.\n",
        "        \"\"\",\n",
        "        \"question\": \"Who coined the term machine learning?\",\n",
        "        \"expected_answer\": \"Arthur Samuel\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\"\"\n",
        "        The Great Wall of China is a series of fortifications that were built across the historical\n",
        "        northern borders of ancient Chinese states and Imperial China as protection against various\n",
        "        nomadic groups from the Eurasian Steppe. The Great Wall construction started as early as\n",
        "        the 7th century BC and continued until 1878 in the Qing dynasty.\n",
        "        \"\"\",\n",
        "        \"question\": \"What was the purpose of the Great Wall of China?\",\n",
        "        \"expected_answer\": \"protection against various nomadic groups from the Eurasian Steppe\"\n",
        "    }\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    context = example[\"context\"].strip()\n",
        "    question = example[\"question\"]\n",
        "    expected = example[\"expected_answer\"]\n",
        "\n",
        "    answer = answer_question(context, question, model, tokenizer)\n",
        "\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Expected Answer: {expected}\")\n",
        "    print(f\"Model Answer: {answer}\")\n",
        "    print(f\"Context (first 150 chars): {context[:150]}...\")\n",
        "\n",
        "# 16. Batch Evaluation\n",
        "def evaluate_on_samples(model, tokenizer, dataset_samples, num_samples=20):\n",
        "    results = []\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset_samples))):\n",
        "        sample = dataset_samples[i]\n",
        "        context = sample['context']\n",
        "        question = sample['question']\n",
        "        true_answer = sample['answers']['text'][0] if len(sample['answers']['text']) > 0 else \"\"\n",
        "\n",
        "        predicted_answer = answer_question(context, question, model, tokenizer)\n",
        "\n",
        "        # Simple exact match\n",
        "        is_exact_match = predicted_answer.strip().lower() == true_answer.strip().lower()\n",
        "\n",
        "        # Calculate F1\n",
        "        def compute_simple_f1(pred, gold):\n",
        "            pred_tokens = pred.lower().split()\n",
        "            gold_tokens = gold.lower().split()\n",
        "\n",
        "            common = set(pred_tokens) & set(gold_tokens)\n",
        "\n",
        "            if len(common) == 0:\n",
        "                return 0\n",
        "\n",
        "            precision = len(common) / len(pred_tokens)\n",
        "            recall = len(common) / len(gold_tokens)\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            return f1\n",
        "\n",
        "        f1_score = compute_simple_f1(predicted_answer, true_answer)\n",
        "\n",
        "        results.append({\n",
        "            \"id\": i,\n",
        "            \"question\": question,\n",
        "            \"true_answer\": true_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"exact_match\": is_exact_match,\n",
        "            \"f1\": f1_score\n",
        "        })\n",
        "\n",
        "    # Calculate aggregate metrics\n",
        "    exact_match_rate = sum([r[\"exact_match\"] for r in results]) / len(results)\n",
        "    avg_f1 = np.mean([r[\"f1\"] for r in results])\n",
        "\n",
        "    print(f\"\\nEvaluation on {len(results)} samples:\")\n",
        "    print(f\"Exact Match Rate: {exact_match_rate:.2%}\")\n",
        "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "    # Show some examples\n",
        "    print(\"\\nSample predictions:\")\n",
        "    for i in range(min(3, len(results))):\n",
        "        r = results[i]\n",
        "        print(f\"\\n{i+1}. Question: {r['question']}\")\n",
        "        print(f\"   True: {r['true_answer']}\")\n",
        "        print(f\"   Pred: {r['predicted_answer']}\")\n",
        "        print(f\"   Exact Match: {r['exact_match']}, F1: {r['f1']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Evaluating on SQuAD validation samples\")\n",
        "print(\"=\"*80)\n",
        "eval_results = evaluate_on_samples(model, tokenizer, dataset['validation'], 10)"
      ]
    }
  ]
}