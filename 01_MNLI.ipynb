{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraniayudyaa/finnalterm-dl/blob/main/01_MNLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE-TUNING HUGGINGFACE MODELS (MNLI)**"
      ],
      "metadata": {
        "id": "DnfNtr6uyAlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Setup dan Instalasi**"
      ],
      "metadata": {
        "id": "oC-kjtKQyDf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup dan Instalasi\n",
        "!pip install transformers datasets torch scikit-learn pandas numpy matplotlib seaborn evaluate\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import load_dataset, load_metric\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "SLwZmoUYyQuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load Dataset**"
      ],
      "metadata": {
        "id": "nkG7jTZ8yRYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Dataset - MNLI (Multi-Genre Natural Language Inference)\n",
        "print(\"Loading MNLI dataset from GLUE...\")\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)\n",
        "print(f\"Train samples: {len(dataset['train'])}\")\n",
        "print(f\"Validation matched samples: {len(dataset['validation_matched'])}\")\n",
        "print(f\"Validation mismatched samples: {len(dataset['validation_mismatched'])}\")\n",
        "print(f\"Test matched samples: {len(dataset['test_matched'])}\")\n",
        "print(f\"Test mismatched samples: {len(dataset['test_mismatched'])}\")\n",
        "\n",
        "# 3. Examine data samples\n",
        "print(\"\\nSample data from training set:\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Premise: {sample['premise']}\")\n",
        "print(f\"Hypothesis: {sample['hypothesis']}\")\n",
        "print(f\"Label: {sample['label']} ({sample['label']} = {['entailment', 'neutral', 'contradiction'][sample['label']]})\")\n",
        "print(f\"Genre: {sample['genre']}\")\n",
        "\n",
        "# 4. Analyze dataset statistics\n",
        "def analyze_mnli_dataset(dataset_split, split_name):\n",
        "    print(f\"\\n{split_name} Statistics:\")\n",
        "\n",
        "    # Label distribution\n",
        "    labels = dataset_split['label']\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    label_names = ['entailment', 'neutral', 'contradiction']\n",
        "\n",
        "    print(f\"Label distribution:\")\n",
        "    for label, count in zip(unique, counts):\n",
        "        print(f\"  {label_names[label]}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "    # Genre distribution\n",
        "    if 'genre' in dataset_split.features:\n",
        "        genres = dataset_split['genre']\n",
        "        unique_genres, genre_counts = np.unique(genres, return_counts=True)\n",
        "        print(f\"\\nGenre distribution:\")\n",
        "        for genre, count in zip(unique_genres, genre_counts):\n",
        "            print(f\"  {genre}: {count} samples ({count/len(genres)*100:.1f}%)\")\n",
        "\n",
        "    # Text length statistics\n",
        "    premise_lengths = [len(p.split()) for p in dataset_split['premise']]\n",
        "    hypothesis_lengths = [len(h.split()) for h in dataset_split['hypothesis']]\n",
        "\n",
        "    print(f\"\\nText length statistics:\")\n",
        "    print(f\"  Premise - Avg: {np.mean(premise_lengths):.1f} words, Max: {max(premise_lengths)}\")\n",
        "    print(f\"  Hypothesis - Avg: {np.mean(hypothesis_lengths):.1f} words, Max: {max(hypothesis_lengths)}\")\n",
        "\n",
        "analyze_mnli_dataset(dataset['train'], 'Training Set')\n",
        "analyze_mnli_dataset(dataset['validation_matched'], 'Validation Matched Set')"
      ],
      "metadata": {
        "id": "w8uXS6vRyUcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Preprocessing Data**"
      ],
      "metadata": {
        "id": "jV-HAtn9yT9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Preprocessing dan Tokenization untuk NLI\n",
        "MODEL_NAME = \"bert-base-uncased\"  # Bisa diganti dengan distilbert-base-uncased atau roberta-base\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def preprocess_mnli(examples):\n",
        "    # Format untuk NLI: \"[CLS] premise [SEP] hypothesis [SEP]\"\n",
        "    return tokenizer(\n",
        "        examples['premise'],\n",
        "        examples['hypothesis'],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        truncation_strategy='only_first'  # Truncate premise jika terlalu panjang\n",
        "    )\n",
        "\n",
        "print(\"\\nTokenizing dataset...\")\n",
        "tokenized_datasets = dataset.map(preprocess_mnli, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['premise', 'hypothesis', 'idx', 'genre'])\n",
        "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
        "\n",
        "# 6. Split training data untuk validation\n",
        "train_val_split = tokenized_datasets['train'].train_test_split(test_size=0.1, seed=42)\n",
        "tokenized_datasets['train'] = train_val_split['train']\n",
        "tokenized_datasets['val'] = train_val_split['test']"
      ],
      "metadata": {
        "id": "7E85r2mEydhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Load Model dan Training**"
      ],
      "metadata": {
        "id": "si30ME9ryeKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Load Model untuk Sequence Classification (3 labels)\n",
        "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=3  # entailment, neutral, contradiction\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# 8. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# 9. Load GLUE metric untuk MNLI\n",
        "metric = load_metric(\"glue\", \"mnli\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Use GLUE metric for additional metrics\n",
        "    result = metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        **result\n",
        "    }\n",
        "\n",
        "# 10. Data Collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# 11. Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"val\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 12. Train Model\n",
        "print(\"\\nTraining model for Natural Language Inference...\")\n",
        "train_result = trainer.train()"
      ],
      "metadata": {
        "id": "-XUHH_2DyhMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Evaluasi**"
      ],
      "metadata": {
        "id": "efVAYq3Ayhv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdNUDqGhx-ar"
      },
      "outputs": [],
      "source": [
        "# 13. Evaluate on Validation Set\n",
        "print(\"\\nEvaluating model on validation set...\")\n",
        "eval_result = trainer.evaluate()\n",
        "print(f\"\\nValidation results:\")\n",
        "for key, value in eval_result.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# 14. Evaluate on both matched and mismatched validation sets\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Evaluation on Official Validation Sets\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Evaluate on validation_matched\n",
        "print(\"\\nEvaluating on validation_matched...\")\n",
        "val_matched_results = trainer.predict(tokenized_datasets['validation_matched'])\n",
        "val_matched_metrics = val_matched_results.metrics\n",
        "print(f\"Validation Matched Accuracy: {val_matched_metrics.get('test_accuracy', 0):.4f}\")\n",
        "\n",
        "# Evaluate on validation_mismatched\n",
        "print(\"\\nEvaluating on validation_mismatched...\")\n",
        "val_mismatched_results = trainer.predict(tokenized_datasets['validation_mismatched'])\n",
        "val_mismatched_metrics = val_mismatched_results.metrics\n",
        "print(f\"Validation Mismatched Accuracy: {val_mismatched_metrics.get('test_accuracy', 0):.4f}\")\n",
        "\n",
        "# 15. Save Model\n",
        "print(\"\\nSaving model...\")\n",
        "trainer.save_model(\"./saved_model_mnli\")\n",
        "tokenizer.save_pretrained(\"./saved_model_mnli\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Visualization Functions\n",
        "def plot_confusion_matrix_nli(y_true, y_pred, labels, title='MNLI Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix_mnli.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history_nli(trainer_state):\n",
        "    if trainer_state.log_history:\n",
        "        history = pd.DataFrame(trainer_state.log_history)\n",
        "\n",
        "        # Plot loss\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        if 'loss' in history.columns:\n",
        "            train_loss = history[history['loss'].notna()]\n",
        "            plt.plot(train_loss['step'], train_loss['loss'], label='Training Loss')\n",
        "        if 'eval_loss' in history.columns:\n",
        "            eval_loss = history[history['eval_loss'].notna()]\n",
        "            plt.plot(eval_loss['step'], eval_loss['eval_loss'], label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Steps')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if 'eval_accuracy' in history.columns:\n",
        "            eval_acc = history[history['eval_accuracy'].notna()]\n",
        "            plt.plot(eval_acc['step'], eval_acc['eval_accuracy'], label='Validation Accuracy', color='green')\n",
        "        plt.title('Validation Accuracy')\n",
        "        plt.xlabel('Steps')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_history_mnli.png')\n",
        "        plt.show()\n",
        "\n",
        "# Get predictions for confusion matrix\n",
        "predictions = np.argmax(val_matched_results.predictions, axis=1)\n",
        "labels = val_matched_results.label_ids\n",
        "\n",
        "# Class names for MNLI\n",
        "class_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nDetailed Classification Report (validation_matched):\")\n",
        "print(classification_report(labels, predictions, target_names=class_names, digits=4))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix_nli(labels, predictions, class_names, 'MNLI - Confusion Matrix (Matched)')\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history_nli(trainer.state)\n",
        "\n",
        "# 17. Inference Function untuk NLI\n",
        "def predict_nli(premise, hypothesis, model, tokenizer, device):\n",
        "    # Tokenize input pair\n",
        "    inputs = tokenizer(\n",
        "        premise,\n",
        "        hypothesis,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(predictions, dim=1).item()\n",
        "    probabilities = predictions[0].cpu().numpy()\n",
        "\n",
        "    # Get class with highest probability\n",
        "    class_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "    # Get top predictions\n",
        "    top3_indices = np.argsort(probabilities)[-3:][::-1]\n",
        "    top3_predictions = [(class_names[i], probabilities[i]) for i in top3_indices]\n",
        "\n",
        "    return {\n",
        "        \"premise\": premise[:100] + \"...\" if len(premise) > 100 else premise,\n",
        "        \"hypothesis\": hypothesis[:100] + \"...\" if len(hypothesis) > 100 else hypothesis,\n",
        "        \"predicted_class\": predicted_class,\n",
        "        \"predicted_label\": class_names[predicted_class],\n",
        "        \"confidence\": probabilities[predicted_class],\n",
        "        \"probabilities\": probabilities,\n",
        "        \"top3_predictions\": top3_predictions,\n",
        "        \"interpretation\": get_nli_interpretation(class_names[predicted_class], probabilities[predicted_class])\n",
        "    }\n",
        "\n",
        "def get_nli_interpretation(label, confidence):\n",
        "    interpretations = {\n",
        "        \"entailment\": \"The hypothesis follows from the premise.\",\n",
        "        \"neutral\": \"The hypothesis might be true given the premise, but it's not necessarily entailed.\",\n",
        "        \"contradiction\": \"The hypothesis contradicts the premise.\"\n",
        "    }\n",
        "\n",
        "    strength = \"\"\n",
        "    if confidence > 0.9:\n",
        "        strength = \" (strong)\"\n",
        "    elif confidence > 0.7:\n",
        "        strength = \" (moderate)\"\n",
        "    else:\n",
        "        strength = \" (weak)\"\n",
        "\n",
        "    return interpretations.get(label, \"Unknown relationship\") + strength\n",
        "\n",
        "# 18. Test Inference dengan berbagai contoh\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Natural Language Inference Test Examples\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_examples = [\n",
        "    {\n",
        "        \"premise\": \"A man is playing guitar on stage.\",\n",
        "        \"hypothesis\": \"A musician is performing.\",\n",
        "        \"expected\": \"entailment\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"The cat is sleeping on the sofa.\",\n",
        "        \"hypothesis\": \"The dog is sleeping on the sofa.\",\n",
        "        \"expected\": \"contradiction\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"She bought apples from the market.\",\n",
        "        \"hypothesis\": \"She purchased fruits.\",\n",
        "        \"expected\": \"neutral\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"All students passed the exam.\",\n",
        "        \"hypothesis\": \"No student failed the exam.\",\n",
        "        \"expected\": \"entailment\"\n",
        "    },\n",
        "    {\n",
        "        \"premise\": \"The restaurant was empty.\",\n",
        "        \"hypothesis\": \"The restaurant was full of customers.\",\n",
        "        \"expected\": \"contradiction\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nTesting NLI predictions:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    result = predict_nli(\n",
        "        example[\"premise\"],\n",
        "        example[\"hypothesis\"],\n",
        "        model,\n",
        "        tokenizer,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Premise: {result['premise']}\")\n",
        "    print(f\"Hypothesis: {result['hypothesis']}\")\n",
        "    print(f\"Predicted: {result['predicted_label']} (Expected: {example['expected']})\")\n",
        "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(f\"Interpretation: {result['interpretation']}\")\n",
        "\n",
        "    # Check if prediction matches expected\n",
        "    match = \"✓\" if result['predicted_label'] == example['expected'] else \"✗\"\n",
        "    print(f\"Correct: {match}\")\n",
        "\n",
        "    print(\"Top 3 predictions:\")\n",
        "    for label, prob in result['top3_predictions']:\n",
        "        print(f\"  - {label}: {prob:.2%}\")\n",
        "\n",
        "# 19. Per-relation analysis\n",
        "def analyze_relation_performance(model, tokenizer, device, test_data, num_samples=50):\n",
        "    \"\"\"Analyze model performance per relation type\"\"\"\n",
        "    relations = {\n",
        "        \"entailment\": {\"correct\": 0, \"total\": 0},\n",
        "        \"neutral\": {\"correct\": 0, \"total\": 0},\n",
        "        \"contradiction\": {\"correct\": 0, \"total\": 0}\n",
        "    }\n",
        "\n",
        "    # Sample data\n",
        "    sample_indices = np.random.choice(len(test_data), min(num_samples, len(test_data)), replace=False)\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        sample = test_data[int(idx)]\n",
        "        premise = sample['premise']\n",
        "        hypothesis = sample['hypothesis']\n",
        "        true_label = sample['label']\n",
        "\n",
        "        result = predict_nli(premise, hypothesis, model, tokenizer, device)\n",
        "        predicted_label = result['predicted_label']\n",
        "\n",
        "        true_label_name = [\"entailment\", \"neutral\", \"contradiction\"][true_label]\n",
        "\n",
        "        relations[true_label_name][\"total\"] += 1\n",
        "        if predicted_label == true_label_name:\n",
        "            relations[true_label_name][\"correct\"] += 1\n",
        "\n",
        "    # Calculate and display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Per-Relation Performance Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for relation, stats in relations.items():\n",
        "        if stats[\"total\"] > 0:\n",
        "            accuracy = stats[\"correct\"] / stats[\"total\"]\n",
        "            print(f\"{relation.capitalize()}: {stats['correct']}/{stats['total']} = {accuracy:.2%}\")\n",
        "        else:\n",
        "            print(f\"{relation.capitalize()}: No samples\")\n",
        "\n",
        "# Analyze performance\n",
        "analyze_relation_performance(model, tokenizer, device, dataset['validation_matched'])\n",
        "\n",
        "# 20. Error Analysis - Contoh salah prediksi\n",
        "def find_error_cases(model, tokenizer, device, test_data, num_cases=5):\n",
        "    \"\"\"Find and analyze error cases\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Error Analysis - Misclassified Examples\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    error_cases = []\n",
        "    sample_indices = np.random.choice(len(test_data), min(100, len(test_data)), replace=False)\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        sample = test_data[int(idx)]\n",
        "        premise = sample['premise']\n",
        "        hypothesis = sample['hypothesis']\n",
        "        true_label = sample['label']\n",
        "        true_label_name = [\"entailment\", \"neutral\", \"contradiction\"][true_label]\n",
        "\n",
        "        result = predict_nli(premise, hypothesis, model, tokenizer, device)\n",
        "\n",
        "        if result['predicted_label'] != true_label_name:\n",
        "            error_cases.append({\n",
        "                \"premise\": premise,\n",
        "                \"hypothesis\": hypothesis,\n",
        "                \"true_label\": true_label_name,\n",
        "                \"predicted_label\": result['predicted_label'],\n",
        "                \"confidence\": result['confidence'],\n",
        "                \"probabilities\": result['probabilities']\n",
        "            })\n",
        "\n",
        "        if len(error_cases) >= num_cases:\n",
        "            break\n",
        "\n",
        "    # Display error cases\n",
        "    for i, case in enumerate(error_cases, 1):\n",
        "        print(f\"\\nError Case {i}:\")\n",
        "        print(f\"Premise: {case['premise']}\")\n",
        "        print(f\"Hypothesis: {case['hypothesis']}\")\n",
        "        print(f\"True: {case['true_label']}, Predicted: {case['predicted_label']}\")\n",
        "        print(f\"Confidence: {case['confidence']:.2%}\")\n",
        "        print(f\"Probabilities: entailment={case['probabilities'][0]:.2%}, \"\n",
        "              f\"neutral={case['probabilities'][1]:.2%}, \"\n",
        "              f\"contradiction={case['probabilities'][2]:.2%}\")\n",
        "\n",
        "# Find and analyze error cases\n",
        "find_error_cases(model, tokenizer, device, dataset['validation_matched'])\n",
        "\n",
        "\n",
        "# 22. Advanced Analysis - Genre-wise performance\n",
        "if 'genre' in dataset['validation_matched'].features:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Genre-wise Performance Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    genres = dataset['validation_matched']['genre']\n",
        "    unique_genres = np.unique(genres)\n",
        "\n",
        "    genre_performance = {}\n",
        "\n",
        "    for genre in unique_genres:\n",
        "        # Get indices for this genre\n",
        "        genre_indices = [i for i, g in enumerate(genres) if g == genre]\n",
        "        genre_samples = dataset['validation_matched'].select(genre_indices)\n",
        "\n",
        "        # Tokenize and predict\n",
        "        tokenized_genre = genre_samples.map(preprocess_mnli, batched=True)\n",
        "        tokenized_genre = tokenized_genre.remove_columns(['premise', 'hypothesis', 'idx', 'genre'])\n",
        "        tokenized_genre = tokenized_genre.rename_column('label', 'labels')\n",
        "\n",
        "        genre_results = trainer.predict(tokenized_genre)\n",
        "        genre_predictions = np.argmax(genre_results.predictions, axis=1)\n",
        "        genre_labels = genre_results.label_ids\n",
        "\n",
        "        accuracy = accuracy_score(genre_labels, genre_predictions)\n",
        "        genre_performance[genre] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"samples\": len(genre_indices)\n",
        "        }\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nGenre Performance:\")\n",
        "    for genre, stats in genre_performance.items():\n",
        "        print(f\"  {genre}: {stats['accuracy']:.4f} accuracy ({stats['samples']} samples)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MNLI Fine-tuning Complete!\")\n",
        "print(f\"Model saved to: ./saved_model_mnli\")\n",
        "print(f\"Inference script: inference_mnli.py\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "QKH2h5t2yuRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}