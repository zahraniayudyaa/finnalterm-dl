{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraniayudyaa/finnalterm-dl/blob/main/03_XSum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINE-TUNING HUGGINGFACE MODELS (XSum)**"
      ],
      "metadata": {
        "id": "ucKOQpXW0119"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Setup dan Instalasi**"
      ],
      "metadata": {
        "id": "ZUrnPE4k03FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup dan Instalasi\n",
        "!pip install transformers datasets torch peft bitsandbytes accelerate trl scipy rouge-score -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "import gc\n",
        "from rouge_score import rouge_scorer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "MvPs6QB31Fl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Load Dataset**"
      ],
      "metadata": {
        "id": "rgW1hHlj1GKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load Dataset - XSum\n",
        "print(\"Loading XSum dataset...\")\n",
        "dataset = load_dataset(\"EdinburghNLP/xsum\")\n",
        "\n",
        "print(\"\\nDataset structure:\")\n",
        "print(dataset)\n",
        "print(f\"Train samples: {len(dataset['train'])}\")\n",
        "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
        "print(f\"Test samples: {len(dataset['test'])}\")\n",
        "\n",
        "# 3. Examine data\n",
        "print(\"\\nSample from training set:\")\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Document (first 200 chars): {sample['document'][:200]}...\")\n",
        "print(f\"Summary: {sample['summary']}\")\n",
        "print(f\"ID: {sample['id']}\")\n",
        "\n",
        "print(f\"\\nDocument length: {len(sample['document'].split())} words\")\n",
        "print(f\"Summary length: {len(sample['summary'].split())} words\")\n",
        "\n",
        "# 4. Configure 4-bit quantization untuk menghemat memory\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ],
      "metadata": {
        "id": "oSJ4dtmc1IDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Load Model**"
      ],
      "metadata": {
        "id": "tHNTnNbX1JFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Load Model dan Tokenizer - Phi-2\n",
        "MODEL_NAME = \"microsoft/phi-2\"\n",
        "\n",
        "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "    # Tambahkan padding token jika tidak ada\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    print(\"Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Fallback to smaller model if Phi-2 fails\n",
        "    MODEL_NAME = \"facebook/opt-1.3b\"\n",
        "    print(f\"Falling back to: {MODEL_NAME}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "# Set model config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# 6. Prepare Model for PEFT (Parameter-Efficient Fine-Tuning)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 7. Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # LoRA rank\n",
        "    lora_alpha=32,  # LoRA alpha\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"],  # Target modules for Phi-2/OPT\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# 8. Preprocess Function untuk Summarization\n",
        "def preprocess_xsum(examples):\n",
        "    prompts = []\n",
        "\n",
        "    for doc, summary in zip(examples['document'], examples['summary']):\n",
        "        # Format instruction untuk summarization\n",
        "        prompt = f\"### Document:\\n{doc}\\n\\n### Summary:\\n{summary}\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        prompts,\n",
        "        truncation=True,\n",
        "        max_length=1024,  # Batasi panjang untuk memory\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    # Untuk causal LM, labels sama dengan input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "uU6wxGBE1RPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Preprocessing Data**"
      ],
      "metadata": {
        "id": "wPumCfWF1Tl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Process Dataset\n",
        "print(\"\\nPreprocessing dataset...\")\n",
        "# Gunakan subset untuk demo (hapus [:] untuk full training)\n",
        "train_subset = dataset['train'].select(range(2000))  # Small subset for demo\n",
        "val_subset = dataset['validation'].select(range(200))\n",
        "\n",
        "tokenized_train = train_subset.map(preprocess_xsum, batched=True, remove_columns=train_subset.column_names)\n",
        "tokenized_val = val_subset.map(preprocess_xsum, batched=True, remove_columns=val_subset.column_names)\n",
        "\n",
        "print(f\"Training samples: {len(tokenized_train)}\")\n",
        "print(f\"Validation samples: {len(tokenized_val)}\")\n",
        "\n",
        "# 10. Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal language modeling\n",
        ")"
      ],
      "metadata": {
        "id": "1q1yJX1W1VLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Training Model**"
      ],
      "metadata": {
        "id": "HOLs2IJs1YUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_xsum\",\n",
        "    num_train_epochs=2,  # Reduced for demo\n",
        "    per_device_train_batch_size=2,  # Small batch size karena model besar\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
        "    warmup_steps=50,\n",
        "    logging_steps=20,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_8bit\",  # Optimizer untuk 8-bit training\n",
        "    gradient_checkpointing=True,  # Menghemat memory\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "# 12. Initialize Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=1024,\n",
        ")\n",
        "\n",
        "# 13. Train Model\n",
        "print(\"\\nTraining model... (This may take a while)\")\n",
        "trainer.train()\n",
        "\n",
        "# 14. Save Model\n",
        "print(\"\\nSaving model...\")\n",
        "trainer.save_model(\"./saved_model_phi2_xsum\")\n",
        "tokenizer.save_pretrained(\"./saved_model_phi2_xsum\")"
      ],
      "metadata": {
        "id": "I8T2tTzY1Yne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Generate Summary Function\n",
        "def generate_summary(document, model, tokenizer, max_length=150, temperature=0.7):\n",
        "    # Create prompt\n",
        "    prompt = f\"### Document:\\n{document}\\n\\n### Summary:\\n\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512  # Batasi input length\n",
        "    )\n",
        "\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "    attention_mask = inputs.attention_mask.to(model.device)\n",
        "\n",
        "    # Generate summary\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            min_length=30,\n",
        "            length_penalty=2.0,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract summary (part after \"### Summary:\")\n",
        "    if \"### Summary:\" in full_output:\n",
        "        summary = full_output.split(\"### Summary:\")[-1].strip()\n",
        "    else:\n",
        "        summary = full_output\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "6qZhp-HG1msK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Test Generation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Testing Summarization\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ambil beberapa contoh dari test set\n",
        "test_samples = dataset['test'].select(range(3))\n",
        "\n",
        "model.eval()\n",
        "for i, sample in enumerate(test_samples):\n",
        "    document = sample['document']\n",
        "    true_summary = sample['summary']\n",
        "\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Print document preview\n",
        "    print(f\"Document (first 300 chars):\")\n",
        "    print(document[:300] + \"...\" if len(document) > 300 else document)\n",
        "    print(f\"\\nDocument length: {len(document.split())} words\")\n",
        "\n",
        "    # True summary\n",
        "    print(f\"\\nTrue Summary:\")\n",
        "    print(true_summary)\n",
        "    print(f\"Summary length: {len(true_summary.split())} words\")\n",
        "\n",
        "    # Generate summary\n",
        "    generated_summary = generate_summary(document, model, tokenizer, max_length=100)\n",
        "    print(f\"\\nGenerated Summary:\")\n",
        "    print(generated_summary)\n",
        "    print(f\"Generated length: {len(generated_summary.split())} words\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# 17. ROUGE Evaluation Function\n",
        "def evaluate_rouge_summaries(generated_summaries, reference_summaries):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    for gen, ref in zip(generated_summaries, reference_summaries):\n",
        "        scores = scorer.score(ref, gen)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    return {\n",
        "        \"rouge1\": np.mean(rouge1_scores),\n",
        "        \"rouge2\": np.mean(rouge2_scores),\n",
        "        \"rougeL\": np.mean(rougeL_scores)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Xd70bfRt1kmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Evaluasi**"
      ],
      "metadata": {
        "id": "cqmwTNOp1g_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtJ4wMpV0jeo"
      },
      "outputs": [],
      "source": [
        "# 18. Evaluate on Validation Set\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROUGE Evaluation on Validation Set\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ambil subset dari validation set untuk evaluasi\n",
        "eval_samples = dataset['validation'].select(range(10))\n",
        "\n",
        "references = []\n",
        "generated = []\n",
        "\n",
        "print(\"\\nEvaluating summaries...\")\n",
        "for i, sample in enumerate(eval_samples):\n",
        "    document = sample['document']\n",
        "    true_summary = sample['summary']\n",
        "\n",
        "    # Generate summary\n",
        "    summary = generate_summary(document, model, tokenizer, max_length=100)\n",
        "\n",
        "    references.append(true_summary)\n",
        "    generated.append(summary)\n",
        "\n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"  Reference: {true_summary[:80]}...\" if len(true_summary) > 80 else f\"  Reference: {true_summary}\")\n",
        "    print(f\"  Generated: {summary[:80]}...\" if len(summary) > 80 else f\"  Generated: {summary}\")\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = evaluate_rouge_summaries(generated, references)\n",
        "\n",
        "print(f\"\\nROUGE Scores:\")\n",
        "print(f\"  ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"  ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"  ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "# 19. Interactive Demo\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Interactive Summarization Demo\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def interactive_demo():\n",
        "    print(\"\\nEnter a document to summarize (type 'quit' to exit):\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nEnter your document (or 'quit'):\")\n",
        "        document = input(\"> \")\n",
        "\n",
        "        if document.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        if len(document.strip()) < 50:\n",
        "            print(\"Document too short. Please enter at least 50 characters.\")\n",
        "            continue\n",
        "\n",
        "        print(\"\\nGenerating summary...\")\n",
        "        summary = generate_summary(document, model, tokenizer, max_length=100)\n",
        "\n",
        "        print(f\"\\nGenerated Summary:\")\n",
        "        print(summary)\n",
        "\n",
        "        print(f\"\\nSummary length: {len(summary.split())} words\")\n",
        "        print(\"-\" * 60)"
      ]
    }
  ]
}